---
title: RoBERTa__A_Robustly_Optimized_BERT_Pretraining_Approach
date: 2019-07-26 00:00:00 +0800
categories: ['RoBERTa']
tags: ['RoBERTa']
---

- ğŸ“™Paper: "[Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review](https://www.semanticscholar.org/paper/Unleashing-the-potential-of-prompt-engineering-in-a-Chen-Zhang/595c8d39a6155354fd7d8f62a4441be5c82e68da)"
- ğŸ”‘Public: âœ…
- âš² area: Roberta
- ğŸ“… Date: 2019-07-26
- ğŸ” Taxonomy: fine-tuning
- ğŸ“ #References: 68
